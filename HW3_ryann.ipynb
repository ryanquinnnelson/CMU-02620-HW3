{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attended-coffee",
   "metadata": {},
   "source": [
    "# 1. Question 1 (35 pts)\n",
    "\n",
    "**Consider the following Bayesian networks for a gene regulatory network in plant.**\n",
    "\n",
    "![image.png](img/Q1_Bayesian_Network.png)\n",
    "*Figure 1: Plant gene regulatory networks. An ancestral gene regulatory network (left) and a gene regulatory network modified in evolution (right).*\n",
    "\n",
    "***\n",
    "\n",
    "**The conditional probability distribution for the expression level of gene $i$, denoted by $G_i$, is given as a linear regression model:**\n",
    "\n",
    "$$P(G_i|Pa(G_i))=N(\\beta_{0i} +G_{Pa(Gi)}\\beta_i,\\sigma_i^2),$$\n",
    "\n",
    "**where** \n",
    "- **$Pa(G_i)$ is the parents of $G_i$ in the network.**\n",
    "- **$\\beta_i$ is a vector of regresson coefficients corresponding to genes in $Pa(G_i)$.**\n",
    "- **$\\beta_{0i}$ is an intercept.**\n",
    "- **$\\sigma_i^2$ is the variance.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-darkness",
   "metadata": {},
   "source": [
    "## 1.a. (5 pts)\n",
    "**TODO:**\n",
    "- **Write down the local conditional probability distributions for each of the nodes $G_1,\\ldots, G_7$ in the gray part of the ancestral network in Figure 1.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-turkey",
   "metadata": {},
   "source": [
    "$P(G_1|Pa(G_1))=P(G_1)$\n",
    "\n",
    "$P(G_2|Pa(G_2))=P(G_2)$\n",
    "\n",
    "$P(G_3|Pa(G_3))=P(G_3)$\n",
    "\n",
    "$P(G_4|Pa(G_4))=P(G_4)$\n",
    "\n",
    "$P(G_5|Pa(G_4))=P(G_5)$\n",
    "\n",
    "$P(G_6|Pa(G_6))=P(G_6|G_1,G_2,G_3,G_4,G_5)$\n",
    "\n",
    "$P(G_7|Pa(G_7))=P(G_7|G_6)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-north",
   "metadata": {},
   "source": [
    "## 1.b. (5 pts)\n",
    "**TODO**:\n",
    "- **Circle the nodes in the Markov blanket of node $O$.** \n",
    "- **Circle the nodes in the Markov blanket of node $I$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "relevant-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "## picky to not include O or I?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-blair",
   "metadata": {},
   "source": [
    "### Node O\n",
    "![image.png](img/Q1bi_Bayesian_Network.png)\n",
    "\n",
    "### Node I\n",
    "![image.png](img/Q1bii_Bayesian_Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-generator",
   "metadata": {},
   "source": [
    "## 1.c. (15 pts)\n",
    "**TODO:**\n",
    "- **Answer the following questions about the ancestral network in Figure 1. Provide a brief explanation for your answer.**\n",
    "\n",
    "    - **Are $O$ and $I$ $d$-separated by the red node?**\n",
    "    - **Are $O$ and $I$ $d$-separated by $G_7$?**\n",
    "    - **Are $O$ and $G_7$ $d$-separated by the red node?** \n",
    "    - **Are $O$ and $G_5$ $d$-separated by $G_6$?**\n",
    "    - **Are $O$ and $G_5$ $d$-separated by the red node?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "resistant-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x and y are d-separated given Z \n",
    "# if all possible paths between x and y are blocked\n",
    "\n",
    "# A path between x and y is blocked if \n",
    "# the path includes a node n such that either:\n",
    "#    (1)   (n is not a collider node) AND (n is in Z)\n",
    "#\n",
    "#    (2)   (n is a collider node) AND \n",
    "#          (neither n nor any of its descendants is in Z)\n",
    "\n",
    "#?? n is not a collider node at all or only a collider node of x and y?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-death",
   "metadata": {},
   "source": [
    "### Are $O$ and $I$ $d$-separated by the red node?\n",
    "Yes. \n",
    "\n",
    "Given $Z=\\{red\\ node\\}$, we have a path between $O$ and $I$ consisting of the red node. We have a second path between $O$ and $I$ consisting of the red node and purple node and a cerulean node. The red node is in both paths between $O$ and $I$. The red node is in $Z$ and the red node is not a collider node of $O$ and $I$. This fulfills criteria 1 of d-separation.\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "### Are $O$ and $I$ $d$-separated by $G_7$?\n",
    "No. \n",
    "\n",
    "Given $Z=\\{G_7\\}$, we have a path between $O$ and $I$ consisting of the red node. We have a second path between $O$ and $I$ consisting of the red node and purple node and a cerulean node. $G_7$ is not on a path between $O$ and $I$, so neither criteria 1 nor criteria 2 cannot be fulfilled.\n",
    "\n",
    "***\n",
    "\n",
    "### Are $O$ and $G_7$ $d$-separated by the red node?\n",
    "No. \n",
    "\n",
    "Given $Z=\\{red\\ node\\}$, we have one path between $G_7$ and $O$ consisting of the red node. The red node is collider node of $G_7$ and $O$, so criteria 1 cannot be fulfilled. The red node is in $Z$, so criteria 2 cannot be fulfilled.\n",
    "\n",
    "***\n",
    "\n",
    "### Are $O$ and $G_5$ $d$-separated by $G_6$?\n",
    "Yes.\n",
    "\n",
    "Given $Z=\\{G_6\\}$, we have one path between $G_5$ and $O$ consisting of $G_6$, $G_7$, and the red node. $G_6$ is not a collider node of $G_5$ and $O$, and $G_6$ is in $Z$, so criteria 1 is fulfilled.\n",
    "\n",
    "?? The red node is a collider node of $G_5$ and $O$, and $G_6$ is not a descendant of the red node, so criteria 2 is fulfilled. \n",
    "\n",
    "\n",
    "### Are $O$ and $G_5$ $d$-separated by the red node?\n",
    "No.\n",
    "\n",
    "Given $Z=\\{red\\ node\\}$, we have one path between $G_5$ and $O$ consisting of $G_6$, $G_7$, and the red node. The red node is a collider node of $G_5$ and $O$, and the red node is in $Z$, so criteria 2 cannot be fulfilled. No other nodes on the path from $G_5$ and $O$ are in $Z$, so criteria 1 cannot be fulfilled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-warrant",
   "metadata": {},
   "source": [
    "## 1.d. (5 pts)\n",
    "**The change of the network structure from left to right in Figure 1 can affect the local conditional probability distributions for individual nodes.**\n",
    "\n",
    "**TODO:**\n",
    "- **Which node has its local conditional probability distribution affected by this structural change?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-doctrine",
   "metadata": {},
   "source": [
    "The green node into which the new connection is made will have its local conditional probability distribution changed. It used to have the purple node and cerulean node and cyan node as parents, but now has the purple node and cerulean node and orange node as parents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-package",
   "metadata": {},
   "source": [
    "## 1.e. (5 pts)\n",
    "\n",
    "**TODO:**\n",
    "- **Assume $N$ samples are provided as training data. Describe how you would perform MLE to estimate the parameters of the ancestral network in Figure 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-making",
   "metadata": {},
   "source": [
    "### Start\n",
    "With MLE, we want to maximize the probability of observing the data given Bayesian network parameters:\n",
    "\n",
    "$$\\theta_{MLE}=argmax_{\\theta} \\log P(data|\\theta)$$\n",
    "\n",
    "We have a known graph structure, so we only need to learn the probability distribution.\n",
    "\n",
    "\n",
    "### Expression before taking the derivative\n",
    "MLE starts by calculating the product of the joint probability of all nodes in the graph over all samples, then taking the log of that to get the log likelihood:\n",
    "\n",
    "$$\\ell(p)=log \\prod_{i=1}^N P(O^i, I^i, G_1^i, G_2^i,G_3^i,G_4^i,G_5^i,G_6^i, G_7^i, \\ldots)$$\n",
    "\n",
    "Multiplication inside the log is equivalent to the sum of the log of each of the multiplied terms, so we can replace the product with a summation:\n",
    "\n",
    "$$\\ell(p)=\\sum_{i=1}^N \\log P(O^i, I^i, G_1^i, G_2^i,G_3^i,G_4^i,G_5^i,G_6^i, G_7^i, \\ldots)$$\n",
    "\n",
    "The we use the chain rule to redefine the joint probability in terms of local conditional probabilities:\n",
    "\n",
    "$$\\ell(p)=\\sum_{i=1}^N \\log P(G_1^i)P(G_2^i)P(G_3^i)P(G_4^i)P(G_5^i)P(G_6^i|G_1^i,G_2^i,G_3^i,G_4^i,G_5^i)\\ldots$$\n",
    "\n",
    "Multiplication inside the log is equivalent to the sum of the log of each of the multiplied terms:\n",
    "\n",
    "$$\\ell(p)=\\sum_{i=1}^N \\log P(G_1^i)+\\log P(G_2^i)+\\log P(G_3^i)+\\log P(G_4^i)+\\ldots$$\n",
    "\n",
    "We can distribute the summation to each of the terms to get:\n",
    "\n",
    "$$\\ell(p)=\\sum_{i=1}^N \\log P(G_1^i)+\\sum_{i=1}^N\\log P(G_2^i)+\\sum_{i=1}^N\\log P(G_3^i)+\\sum_{i=1}^N\\log P(G_4^i)+\\ldots$$\n",
    "\n",
    "\n",
    "### Taking the Derivative\n",
    "The Bayesian Network parameters we want to estimate are the individual conditional probability distributions. To find these, we take the derivative of the expression with respect to an individual conditional probability distribution, and set it equal to zero to solve for the maximum. This eliminates all of the other terms:\n",
    "\n",
    "$$\\frac{d\\ell(p)}{dp_{G_1}}=\\frac{d}{dp_{G_1}}\\left(\\sum_{i=1}^N \\log P(G_1^i)\\right)=\\frac{d}{dp_{G_1}}\\left( \\sum_{i=1}^N \\log  p^{G_1^i}(1-p)^{(1-G_1^i)}\\right)=0$$\n",
    "\n",
    "which can be solved to get the parameter we want:\n",
    "$$\\hat{p}_{G_1}=\\frac{ \\sum_{i=1}^N G_1^i}{N}$$\n",
    "\n",
    "### Counting Exercise\n",
    "\n",
    "We estimate each of the individual conditional probability parameters by summing over the columns of the conditional probability for the $N$ samples. For example, with $P(G_1)$ we find column $G_1$, sum up the number of samples with $G_1=1$, then divide by $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-analyst",
   "metadata": {},
   "source": [
    "# 2. Question 2 (25 pts)\n",
    "**Consider a Gaussian graphical model $N(0,\\Theta^{-1})$, where $\\Theta$ is a 24 × 24 matrix, with the following undirected graph structure over 24 genes for BRCA gene regulation.**\n",
    "![image.png](img/Q2_Gaussian_graphical_model.png)\n",
    "*Figure 2: Gaussian graphical model*\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "particular-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?? Markov blanket includes only neighbors of BRCA1?\n",
    "# ?? Theta^-1 is covariance matrix or Theta is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-signal",
   "metadata": {},
   "source": [
    "## 2.a. (5 pts)\n",
    "**TODO:**\n",
    "- **What are the genes in the Markov blanket of BRCA1?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-jacob",
   "metadata": {},
   "source": [
    "Neighbors of BRCA1:\n",
    "- TFAP2C\n",
    "- THBS1\n",
    "- CDK2\n",
    "- MSH2\n",
    "- CKS2\n",
    "- CENPE\n",
    "- BARD1\n",
    "- KPNA2\n",
    "- BRAP\n",
    "- ESR1\n",
    "- ACAT2\n",
    "- FANCA\n",
    "- NCAPH\n",
    "- BRIP1\n",
    "- CHEK1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-exposure",
   "metadata": {},
   "source": [
    "## 2.b. (5 pts)\n",
    "**TODO**:\n",
    "-  **Are BRCA1 and ELOVL5 conditionally independent given AQP1 and CDK2?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-processing",
   "metadata": {},
   "source": [
    "Yes.\n",
    "\n",
    "AQP1 is the only neighbor of ELOVL5, so AQP1 blocks all paths between ELOVL5 and BRCA1. Given AQP1, ELOVL5 is conditionally independent of all other nodes in the graph (including BRCA1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-pharmacology",
   "metadata": {},
   "source": [
    "## 2.c. (5 pts)\n",
    "**TODO:**\n",
    "- **Explain how you would obtain the marginal distribution of BRCA1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-counter",
   "metadata": {},
   "source": [
    "To obtain the marginal distribution $p(BRCA1)$, start by finding the joint distribution over all nodes in the graph.\n",
    "\n",
    "$$p(BRCA1, CDK2, AQP1, ELOVL5, \\ldots)=N(\\mu,\\Sigma)$$\n",
    "\n",
    "From this, we simply extract the parameters for BRCA1:\n",
    "\n",
    "$$\\mu=\\begin{bmatrix}\n",
    "    \\mu_{BRCA1}\\\\\n",
    "    \\mu_{CDK2}\\\\\n",
    "    \\mu_{AQP1}\\\\\n",
    "    \\vdots\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$\\Sigma=\\begin{bmatrix}\n",
    "    \\sigma_{BRCA1}^2 &  \\ldots \\\\\n",
    "   \\vdots      & \\ddots\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "and we get\n",
    "\n",
    "$p(BRCA1)=N(\\mu_{BRCA1},\\sigma^2_{BRCA1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-marina",
   "metadata": {},
   "source": [
    "## 2.d. (5 pts)\n",
    "**TODO:**\n",
    "- **Assume you want to infer the conditional probability distribution of BRCA1 given all the other genes. Can you simplify this distribution?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "configured-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "## do you mean simplified to regression model format?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-mobile",
   "metadata": {},
   "source": [
    "![image.png](img/Q2_edge_graph.png)\n",
    "\n",
    "Given\n",
    "\n",
    "\n",
    "$$\\mu=\\begin{bmatrix}\n",
    "    \\mu_1\\\\\n",
    "    \\mu_2\\\\\n",
    "    \\vdots \\\\\n",
    "    \\mu_{24}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$\\Sigma=\\begin{bmatrix}\n",
    "    \\sigma_{1,1}^2 &  \\sigma_{1,2} & \\ldots & \\sigma_{1,24} \\\\\n",
    "    \\sigma_{2,1} &  \\sigma_{2,2}^2 & \\ldots & \\sigma_{2,24} \\\\\n",
    "    \\vdots &  \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\sigma_{24,1} &  \\sigma_{24,2} & \\ldots & \\sigma_{24,24}^2 \\\\\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-standard",
   "metadata": {},
   "source": [
    "### Start\n",
    "If we consider BRCA1 to be $x_1$ with 24 nodes we want $p(x_1|x_2,\\ldots,x_{24})$. \n",
    "\n",
    "We can calculate this as the following complicated distribution:\n",
    "\n",
    "$$p(x_1|x_2,\\ldots,x_{24})=\n",
    "N\\left(\\mu_1 + \\left( x_{2:24}-\\mu_{2:24} \\right)^T\\Sigma^{-1}_{2:24,2:24}\\Sigma_{2:24,1}, \n",
    "\\sigma^2_1 - \\Sigma_{2:24,1}^T\\Sigma^{-1}_{2:24,2:24}\\Sigma_{2:24,1}\\right)$$\n",
    "\n",
    "### Simplification\n",
    "However, we also know the Markov blanket for $x_1$ (i.e. BRCA1). We know that the the conditional distribution of $x_1$ given its Markov blanket is the same as the conditional distribution of $x_1$ given all other genes:\n",
    "\n",
    "$$p(x_1|x_2,\\ldots,x_{24})=p(x_1|x_2,\\ldots,x_{16})$$\n",
    "\n",
    "where $x_2,\\ldots,x_{16}$ are the 15 genes (nodes) in the Markov blanket for BRCA1. We get the following simplified distribution:\n",
    "\n",
    "$$p(x_1|x_2,\\ldots,x_{16})=\n",
    "N\\left(\\mu_1 + \\left( x_{2:16}-\\mu_{2:16} \\right)^T\\Sigma^{-1}_{2:16,2:16}\\Sigma_{2:16,1}, \n",
    "\\sigma^2_1 - \\Sigma_{2:16,1}^T\\Sigma^{-1}_{2:16,2:16}\\Sigma_{2:16,1}\\right)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-forth",
   "metadata": {},
   "source": [
    "## 2.e. (5 pts)\n",
    "**TODO:**\n",
    "- **Assume you are performing an MLE with $l_1$ regularization. As you increase the regularization parameter, how would the graph structure be affected?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-exhibit",
   "metadata": {},
   "source": [
    "L1 regularization favors sparse models with fewer features. Given a gene that exerts a weak influence on another gene (i.e. weakly correlated), L1 regularization would likely reduce the correlation to zero. This effectively cuts the edge connection between those two genes.\n",
    "\n",
    "The resulting graph structure would have many fewer edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-entrance",
   "metadata": {},
   "source": [
    "# 3. Question 3 (40 pts)\n",
    "**TODO**\n",
    "- **Implement the EM algorithm for Gaussian mixture models.**\n",
    "- **Apply this to the expression data for mouse HIP brain tissue from Homework 2. Use only the first 10 mice (the first 10 rows in the data matrix) and cluster the genes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-isolation",
   "metadata": {},
   "source": [
    "### Parts of EM\n",
    "\n",
    "\n",
    "#### Data\n",
    "\n",
    "N samples $\\{X^1,\\ldots,X^N\\}$\n",
    "\n",
    "Each sample has J features: $X^n=\\{x_1,\\ldots,x_J\\}$\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "#### Probability Model\n",
    "\n",
    "Each of $K$ clusters is modeled as a multivariate Gaussian distribution. A single probability model is formed for a mixture of $K$ Gaussians by weighting each multivariate Gaussian distribution (mixing component) by its cluster size (mixing proportion):\n",
    "\n",
    "$$p(X)=\\sum_{k=1}^K p(X|c=k)p(c=k)$$\n",
    "\n",
    "where\n",
    "- $p(c=k)=\\pi_k$ for $k=1,\\ldots,K$\n",
    "- $p(X|c=k) =N(\\mu_k,\\Sigma_k)$ for $k=1,\\ldots,K$\n",
    "\n",
    "The parameters to learn are $\\pi_k$ and $\\{\\mu_k, \\Sigma_k\\}$ for $k=1,\\ldots,K$\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "#### Inference\n",
    "\n",
    "Assuming model is given, if we want to infer cluster label $c$ for a given sample $x$, we can use Bayes rule:\n",
    "\n",
    "$$p(c=k|X)=\\frac{p(X|c=k)p(c=k)}{p(X)}=\n",
    "\\frac{p(X|c=k)p(c=k)}{\\sum_{m=1}^K p(X,c=m)}=\n",
    "\\frac{p(X|c=k)p(c=k)}{\\sum_{m=1}^K p(X|c=m)p(c=m)}$$\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "#### MLE (fully observed data, c is observed)\n",
    "Log likelihood:\n",
    "\n",
    "$$\\ell(p)=argmax_\\theta \\sum_{n=1}^N \\log p( X^n, c^n)$$\n",
    "\n",
    "which becomes\n",
    "$$\\ell(p)=\\sum_{n=1}^N \\log p(X^n|c^n) + \\sum_{n=1}^N \\log p(c^n)$$\n",
    "\n",
    "where\n",
    "- $N$ is the number of samples\n",
    "- $p(c=k)=\\pi_k$ \n",
    "- $p(X^n|c^n=k)=N(\\mu_k, \\Sigma_k)$\n",
    "\n",
    "\n",
    "Taking the derivative of the log likelihood function for a given parameter and setting equal to zero gives us the following derivations:\n",
    "$$\\hat{\\pi}_k=\\frac{N_k}{N}$$\n",
    "\n",
    "where $N_k$ is the number of samples in cluster $k$  for $k=1,\\ldots,K$\n",
    "\n",
    "\n",
    "\n",
    "$$\\hat{\\mu}_k =\\frac{\\sum_{n=1}^N X^n I(c^n=k)}{N_k}$$ \n",
    "\n",
    "where $I(c^n=k)$ is an indicator function equal to 1 if $c^n=k$ and 0 otherwise.\n",
    "\n",
    "\n",
    "$$\\hat{\\Sigma}_k=\\frac{\\sum_{n=1}^N (X^n-\\mu_k)(X^n-\\mu_k)^T I(c^n=k)}{N_k}$$\n",
    "\n",
    "***\n",
    "\n",
    "#### EM (partial observed data, c is not known)\n",
    "\n",
    "$$\\ell(p)=\\log \\prod_{n=1}^N p(X^n)$$\n",
    "becomes\n",
    "$$\\ell(p)=\\sum_{n=1}^N \\log p(X^n)$$\n",
    "which expands to\n",
    "$$\\ell(p)=\\sum_{n=1}^N \\log \\left( \\sum_{k=1}^K p(X^n, c^n=k) \\right)$$\n",
    "which factorizes to\n",
    "$$\\ell(p)=\\sum_{n=1}^N \\log \\left(\\sum_{k=1}^K p(X^n|c^n=k)p(c^n=k) \\right)$$\n",
    "\n",
    "Taking the derivative of this is difficult. We can go about this a different way by maximizing the *complete data log likelihood* for N samples by using expectation:\n",
    "\n",
    "$$E_{p(c^n|X^n)}\\left[ \\sum_{n=1}^N \\log p( X^n, c^n) \\right]$$\n",
    "\n",
    "We perform MLE of this equation to derive parameters.\n",
    "\n",
    "#### EM (e step)\n",
    "Inference step. We calculate the probability of each of $K$ clusters, given the data. We augment our data matrix with the soft assignment columns.\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    x_1^1 & x_2^1 & \\ldots & x_J^1 \\\\\n",
    "    x_1^2 & x_2^2 & \\ldots & x_J^2 \\\\\n",
    "    \\vdots & \\vdots & \\ldots & \\vdots \\\\\n",
    "        x_1^N & x_2^N & \\ldots & x_J^N \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    p(c^1=k|X^1) & \\ldots & p(c^1=K|X^1) \\\\\n",
    "    p(c^2=k|X^2) & \\ldots & p(c^2=K|X^2) \\\\\n",
    "    \\vdots & \\ldots & \\vdots \\\\\n",
    "    p(c^N=k|X^N) & \\ldots & p(c^N=K|X^N) \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "***\n",
    "\n",
    "#### EM (m step)\n",
    "MLE for parameter estimation, using the augmented data matrix:\n",
    "\n",
    "$$p(c=k)=\\frac{\\sum_{n=1}^N p(c^n=k|X^n)}{N}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\mu_k=\\frac{\\sum_{n=1}^N X^n p(c^n=k|X^n)}{\\sum_{n=1}^N p(c^n=k|X^n)}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\Sigma_k=\\frac{\\sum_{n=1}^N (X^n-\\mu_k)(X^n-\\mu_k)^Tp(c^N=k|X^n)}{\\sum_{n=1}^N p(c^n=k|X^n)}$$\n",
    "\n",
    "***\n",
    "\n",
    "#### EM (repeat)\n",
    "We start with an initial e step with randomized values. Then we repeat (m step, e step) until convergence. (Convergence is when no sample changes cluster assignment after the m step.)\n",
    "\n",
    "?? What is convergence criteria\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "#### Oracle function\n",
    "We can ensure we are on the right track by checking that log-likelihood of the data increases with each iteration:\n",
    "\n",
    "$$\\sum_{n=1}^N \\log{p(X^n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-cream",
   "metadata": {},
   "source": [
    "## 3.a. (10 pts)\n",
    "**TODO:**\n",
    "- **Assume $K = 3$ clusters. Use the initialization provided in Homework 2 to initialize the means (use the first 10 rows for 10 mice).** \n",
    "- **Use a 10×10 diagonal matrix with 1.0 along the diagonals to initialize all covariance matrices.** \n",
    "- **Use [0.3, 0.3, 0.4] to initialize the mixing proportions.** \n",
    "- **Plot the log-likelihood of data $\\log{P(Data)} = \\sum_{n=1}^N \\log{p(X^n)}$, where $N$ is the number of genes, over iterations.** \n",
    "\n",
    "**(Hint: The data log-likelihood should always go up over iterations. If this values goes down even slightly, this means your code has a bug!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-contract",
   "metadata": {},
   "source": [
    "## 3.b. (10 pts)\n",
    "**TODO:**\n",
    "- **Using the model you estimated in (a) above, compute the probability of the first gene to belong to each of the three clusters.**\n",
    "- **Bonus question: do this for all genes and examine the cluster memberships.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-treasure",
   "metadata": {},
   "source": [
    "## 3.c. (10 pts)\n",
    "**TODO:**\n",
    "- **For $K = 3$, try 10 different random initializations for all parameters. Answer these questions:**\n",
    "    - **What is the data log-likelihood at convergence for each initialization?**\n",
    "    - **Which one do you think was the best initialization?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-glenn",
   "metadata": {},
   "source": [
    "## 3.d. (10 pts)\n",
    "**TODO:**\n",
    "- **Run the EM algorithm, assuming $K = 3, \\ldots, 10$ clusters.**\n",
    "- **Plot the log-likelihood of the data across different values for $K$.** \n",
    "- **What do you think is the best choice for the number of clusters?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "effective-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from packages.mixture.GaussianMixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "latest-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([[6, 4],\n",
    "#               [4, 1],\n",
    "#               [1, 2],\n",
    "#               [2, 1]])\n",
    "\n",
    "# gm = GaussianMixture(K=3, epsilon=1e-4)\n",
    "# scores = gm.fit_and_score(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "orange-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stopped-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3 0.3 0.4]\n",
      " [0.3 0.3 0.4]\n",
      " [0.3 0.3 0.4]\n",
      " [0.3 0.3 0.4]]\n",
      "[0.3, 0.3, 0.4]\n",
      "[array([3.25, 2.  ]), array([3.25, 2.  ]), array([3.25, 2.  ])]\n",
      "[array([[3.687501, 1.5     ],\n",
      "       [1.5     , 1.500001]]), array([[3.687501, 1.5     ],\n",
      "       [1.5     , 1.500001]]), array([[3.687501, 1.5     ],\n",
      "       [1.5     , 1.500001]])]\n",
      "-13.727957160354864\n",
      "[0.30000000000000004, 0.30000000000000004, 0.3999999999999999]\n",
      "[array([3.25, 2.  ]), array([3.25, 2.  ]), array([3.25, 2.  ])]\n",
      "[array([[3.687501, 1.5     ],\n",
      "       [1.5     , 1.500001]]), array([[3.687501, 1.5     ],\n",
      "       [1.5     , 1.500001]]), array([[3.687501, 1.5     ],\n",
      "       [1.5     , 1.500001]])]\n",
      "-13.727957160354864\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[6, 4],\n",
    "              [4, 1],\n",
    "              [1, 2],\n",
    "              [2, 1]])\n",
    "A_init = np.array([0.3,0.3,0.4])\n",
    "\n",
    "gm = GaussianMixture(K=3, epsilon=1e-4, A_init = A_init)\n",
    "scores = gm.fit_and_score(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
